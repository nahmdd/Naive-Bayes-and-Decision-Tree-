# -*- coding: utf-8 -*-
"""Copy of Mining2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1imFJZljgskujzDtzZX5b4dSZdSiQh9Qq
"""



from google.colab import files


uploaded = files.upload()

import pandas as pd
from sklearn.preprocessing import OneHotEncoder, LabelEncoder
import numpy as np

# 1. Data Preprocessing
data = pd.read_csv("Bank_dataset.csv")
percentageOfData = float(input("Enter the percentage of total data to be used (e.g., 70): ")) / 100

# 2. Sample the Data
DataSampled = data.sample(frac=percentageOfData , random_state=42)

# 3. Feature Extraction
cat_features = ['job', 'marital', 'education', 'default', 'housing', 'loan']
num_features = ['age', 'balance']

# One-hot encoding for categorical features
encoder = OneHotEncoder(drop='first')
encoder.fit(data[cat_features])
encoded_features = encoder.transform(DataSampled[cat_features]).toarray()

t_features = np.concatenate((encoded_features, DataSampled[num_features].values), axis=1)
t_labels = DataSampled['y']

# Label encoding for target variable
labelEncoder = LabelEncoder()
t_labels = labelEncoder.fit_transform(t_labels)


# Gaussian Naive Bayes Classifier
class GNB:
    def __init__(self):
        self.priors = {}
        self.means = {}
        self.variances = {}

    def fit(self, A, B):
        classes = np.unique(B)
        for clas in classes:
            A_clas = A[B == clas]
            self.priors[clas] = len(A_clas) / len(A)
            self.means[clas] = np.mean(A_clas, axis=0)
            self.variances[clas] = np.var(A_clas, axis=0)

    def predict(self, X):
        preds = []
        for x in X:
            scores = []
            for clas in self.priors:
                prior = self.priors[clas]
                mean = self.means[clas]
                variance = self.variances[clas]
                prob = np.exp(-((x - mean) ** 2) / (2 * variance)) / np.sqrt(2 * np.pi * variance)
                scores.append(prior * np.prod(prob))
            chosen_class = max(self.priors, key=lambda cals: scores[cals])
            preds.append(chosen_class)
        return preds



class Node:
    def __init__(self):
        self.is_leaf = False
        self.label = None
        self.feature_index = None
        self.threshold = None
        self.left = None
        self.right = None


class DTC:
    def __init__(self, max_depth=None, min_samples_split=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.tree = None


    def fitFun(self, A, B):
        self.tree = self.build_tree(A, B)

    def predictFun(self, X):
        preds = []
        for x in X:
            chosen_class = self.predictInstanceFun(x, self.tree)
            preds.append(chosen_class)
        return preds

    def predictInstanceFun(self, x, node):
        if node.is_leaf:
            return node.label
        if x[node.feature_index] <= node.threshold:
            return self.predictInstanceFun(x, node.left)
        else:
            return self.predictInstanceFun(x, node.right)

    def build_tree(self, X, y, depth=0):
        node = Node()

        if depth == self.max_depth or len(np.unique(y)) == 1 or len(y) < self.min_samples_split:
            node.is_leaf = True
            node.label = self.get_majority_class(y)
            return node

        best_feature, best_threshold = self.findBestSplit(X, y)
        node.feature_index = best_feature
        node.threshold = best_threshold

        left_indices = X[:, best_feature] <= best_threshold
        right_indices = X[:, best_feature] > best_threshold

        node.left = self.build_tree(X[left_indices], y[left_indices], depth + 1)
        node.right = self.build_tree(X[right_indices], y[right_indices], depth + 1)

        return node

    def findBestSplit(self, X, y):
        best_gini = 1.0
        best_feature = None
        best_threshold = None

        for feature in range(X.shape[1]):
            thresholds = np.unique(X[:, feature])
            for threshold in thresholds:
                gini = self.calculateGini(X, y, feature, threshold)
                if gini < best_gini:
                    best_gini = gini
                    best_feature = feature
                    best_threshold = threshold

        return best_feature, best_threshold

    def calculateGini(self, X, y, feature, threshold):
        l_indices = X[:, feature] <= threshold
        r_indices = X[:, feature] > threshold

        l_labels = y[l_indices]
        r_labels = y[r_indices]

        l_counts = np.bincount(l_labels)
        r_counts = np.bincount(r_labels)

        l_gini = 1.0 - np.sum((l_counts / len(l_labels)) ** 2)
        r_gini = 1.0 - np.sum((r_counts / len(r_labels)) ** 2)

        weighted_gini = (len(l_labels) / len(y)) * l_gini + (len(r_labels) / len(y)) * r_gini

        return weighted_gini

    def get_majority_class(self, y):
        unique_labels, label_counts = np.unique(y, return_counts=True)
        majority_index = np.argmax(label_counts)
        return unique_labels[majority_index]


# 4. Building Classifier Models
bayesian_model = GNB()
bayesian_model.fit(t_features, t_labels)

decision_tree_model = DTC(max_depth=5)
decision_tree_model.fitFun(t_features, t_labels)

# 5. Model Evaluation
testing_data = data.drop(DataSampled.index)  # Use the remaining data for testing
encoded_test_features = encoder.transform(testing_data[cat_features]).toarray()
t_features = np.concatenate((encoded_test_features, testing_data[num_features].values), axis=1)
t_labels = labelEncoder.transform(testing_data['y'])

GNBpredictions = bayesian_model.predict(t_features)
GNBaccuracy = np.mean(GNBpredictions == t_labels)

DTCpredictions = decision_tree_model.predictFun(t_features)
DTCaccuracy = np.mean(DTCpredictions == t_labels)

# 6. Compare Results
print(" GNB Accuracy:", GNBaccuracy)
print("DTC Accuracy:", DTCaccuracy)